{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dynamic Hand Gesture Classification on DHG-14/28 dataset",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONCJUeEjsK9HIMaSJVky7H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bladefinger/100DaysOfCode/blob/main/Dynamic_Hand_Gesture_Classification_on_DHG_14_28_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozwyDUNuEKXC"
      },
      "outputs": [],
      "source": [
        "#! mkdir dataset_shrec2017\n",
        "#! wget http://www-rech.telecom-lille.fr/shrec2017-hand/HandGestureDataset_SHREC2017.tar.gz \n",
        "#! -O SHREC2017.tar.gz tar -xzf SHREC2017.tar.gz -C dataset_shrec2017"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir dataset_dhg1428\n",
        "! wget http://www-rech.telecom-lille.fr/DHGdataset/DHG2016.zip \n",
        "! unzip DHG2016.zip -d dataset_dhg1428"
      ],
      "metadata": {
        "id": "robQsssZFidq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#! mkdir dataset_onlinedhg \n",
        "#! wget http://www-rech.telecom-lille.fr/shrec2017-hand/OnlineDHG.zip\n",
        "#! unzip OnlineDHG.zip -d dataset_onlinedhg"
      ],
      "metadata": {
        "id": "RCVOpjlWFsAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import numpy\n",
        "import pickle\n",
        "from scipy import ndimage as ndimage\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def resize_gestures(input_gestures, final_length=100):\n",
        "    \"\"\"\n",
        "    Resize the time series by interpolating them to the same length\n",
        "\n",
        "    Input:\n",
        "        - input_gestures: list of numpy.ndarray tensors.\n",
        "              Each tensor represents a single gesture.\n",
        "              Gestures can have variable durations.\n",
        "              Each tensor has a shape: (duration, channels)\n",
        "              where duration is the duration of the individual gesture\n",
        "                    channels = 44 = 2 * 22 if recorded in 2D and\n",
        "                    channels = 66 = 3 * 22 if recorded in 3D \n",
        "    Output:\n",
        "        - output_gestures: one numpy.ndarray tensor.\n",
        "              The output tensor has a shape: (records, final_length, channels)\n",
        "              where records = len(input_gestures)\n",
        "                   final_length is the common duration of all gestures\n",
        "                   channels is the same as above \n",
        "    \"\"\"\n",
        "    # please use python3. if you still use python2, important note: redefine the classic division operator / by importing it from the __future__ module\n",
        "    output_gestures = numpy.array([numpy.array([ndimage.zoom(x_i.T[j], final_length / len(x_i), mode='reflect') for j in range(numpy.size(x_i, 1))]).T for x_i in input_gestures])\n",
        "    return output_gestures\n",
        "\n",
        "\n",
        "def load_gestures(dataset='dhg', root='./dataset_dhg1428', version_x='3D', version_y='14', resize_gesture_to_length=100):\n",
        "    \"\"\"\n",
        "    Get the 3D or 2D pose gestures sequences, and their associated labels.\n",
        "\n",
        "    Ouput:\n",
        "        - a tuple of (gestures, labels) or (gestures, labels_14, labels_28)\n",
        "              where gestures is either a numpy.ndarray tensor or\n",
        "                                       a list of numpy.ndarray tensors,\n",
        "                                       depending on if the gestures have been resized or not.\n",
        "              Each tensor represents a single gesture.\n",
        "              Gestures can have variable durations.\n",
        "              Each tensor has a shape: (duration, channels) where channels is either 44 (= 2 * 22) or 66 (=3 * 22)\n",
        "    \"\"\"\n",
        "\n",
        "    # SHREC 2017 (on Google Colab):\n",
        "    # root = '/content/dataset_shrec2017/HandGestureDataset_SHREC2017'\n",
        "    # DHG 14/28 (on Google Colab):\n",
        "    root = '/content/dataset_dhg1428'\n",
        "    if dataset == 'dhg':\n",
        "      assert 'dataset_dhg' in root\n",
        "    if dataset == 'shrec':\n",
        "      assert 'dataset_shrec' in root\n",
        "    \n",
        "    if version_x == '3D':\n",
        "        if dataset == 'dhg':\n",
        "            pattern = root + '/gesture_*/finger_*/subject_*/essai_*/skeleton_world.txt'\n",
        "        elif dataset == 'shrec':\n",
        "            pattern = root + '/gesture_*/finger_*/subject_*/essai_*/skeletons_world.txt'\n",
        "    else:\n",
        "        if dataset == 'dhg':\n",
        "            pattern = root + '/gesture_*/finger_*/subject_*/essai_*/skeleton_image.txt'\n",
        "        elif dataset == 'shrec':\n",
        "            pattern = root + '/gesture_*/finger_*/subject_*/essai_*/skeletons_image.txt'\n",
        "\n",
        "    gestures_filenames = sorted(glob.glob(pattern))\n",
        "    gestures = [numpy.genfromtxt(f) for f in gestures_filenames]\n",
        "    if resize_gesture_to_length is not None:\n",
        "        gestures = resize_gestures(gestures, final_length=resize_gesture_to_length)\n",
        "\n",
        "    labels_14 = [int(filename.split('/')[-5].split('_')[1]) for filename in gestures_filenames]\n",
        "    #labels_28 = [int(filename.split('/')[-4].split('_')[1]) for filename in gestures_filenames]\n",
        "    #labels_28 = [labels_14[idx_gesture] if n_fingers_used == 1 else 14 + labels_14[idx_gesture] for idx_gesture, n_fingers_used in enumerate(labels_28)]\n",
        "\n",
        "    if version_y == '14' or version_y == 14:\n",
        "        return gestures, labels_14\n",
        "    elif version_y == '28' or version_y == 28:\n",
        "        return gestures, labels_28\n",
        "    elif version_y == 'both':\n",
        "        return gestures, labels_14, labels_28\n",
        "\n",
        "\n",
        "def write_data(data, filepath):\n",
        "    \"\"\"Save the dataset to a file. Note: data is a dict with keys 'x_train', ...\"\"\"\n",
        "    with open(filepath, 'wb') as output_file:\n",
        "        pickle.dump(data, output_file)\n",
        "\n",
        "\n",
        "def load_data(filepath='./shrec_data.pckl'):\n",
        "    \"\"\"\n",
        "    Returns hand gesture sequences (X) and their associated labels (Y).\n",
        "    Each sequence has two different labels.\n",
        "    The first label  Y describes the gesture class out of 14 possible gestures (e.g. swiping your hand to the right).\n",
        "    The second label Y describes the gesture class out of 28 possible gestures (e.g. swiping your hand to the right with your index pointed, or not pointed).\n",
        "    \"\"\"\n",
        "    file = open(filepath, 'rb')\n",
        "    data = pickle.load(file, encoding='latin1')  # <<---- change to 'latin1' to 'utf8' if the data does not load\n",
        "    file.close()\n",
        "    return data['x_train'], data['x_test'], data['y_train_14'], data['y_test_14']\n",
        "    #return data['x_train'], data['x_test'], data['y_train_14'], data['y_train_28'], data['y_test_14'], data['y_test_28']\n"
      ],
      "metadata": {
        "id": "L2MdFB0rF22N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# Step 3. Save the dataset(s) you need\n",
        "# ---------------------------------------------------------\n",
        "# Example A: 2D version of the SHREC gestures, untouched, and only the 14-label version of the labels\n",
        "# x_2d_shrec, y_shrec_14 = load_gestures(dataset='shrec',\n",
        "#                                        root='/tmp/dataset_shrec2017/HandGestureDataset_SHREC2017/',\n",
        "#                                        version_x='2D',\n",
        "#                                        version_y='14',\n",
        "#                                        resize_gesture_to_length=None)\n",
        "\n",
        "# Example B: 3D version of the DHG gestures, resized to 100 timesteps\n",
        "#gestures, labels_14, labels_28 = load_gestures(dataset='dhg',\n",
        "gestures, labels_14 = load_gestures(dataset='dhg',\n",
        "                                               root='/content/dataset_dhg1428',\n",
        "                                               version_x='3D',\n",
        "                                               version_y='14',\n",
        "                                               resize_gesture_to_length=100)\n",
        "\n",
        "# Split the dataset into train and test sets if you want:\n",
        "x_train, x_test, y_train_14, y_test_14 = train_test_split(gestures, labels_14)\n",
        "#x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = train_test_split(gestures, labels_14, labels_28)\n",
        "#x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = train_test_split(gestures, labels_14, labels_28, test_size=0.50)\n",
        "#x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = train_test_split(gestures, labels_14, labels_28, test_size=0.50)\n",
        "\n",
        "# Save the dataset\n",
        "data = {\n",
        "    'x_train': x_train,\n",
        "    'x_test': x_test,\n",
        "    'y_train_14': y_train_14,\n",
        "    #'y_train_28': y_train_28,\n",
        "    'y_test_14': y_test_14,\n",
        "    #'y_test_28': y_test_28\n",
        "}\n",
        "write_data(data, filepath='dhg_data.pckl')"
      ],
      "metadata": {
        "id": "3f31fbxaGZPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = load_data('dhg_data.pckl')\n",
        "#x_train, x_test, y_train_14, y_test_14, = load_data('dhg_data.pckl')\n",
        "x_train, x_test, y_train, y_test = load_data('dhg_data.pckl')\n"
      ],
      "metadata": {
        "id": "8DOl7_krGgLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv3D, MaxPool3D, Flatten, Dense\n",
        "from keras.layers import Dropout, Input, BatchNormalization\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from plotly.offline import iplot, init_notebook_mode\n",
        "from keras.losses import categorical_crossentropy\n",
        "#from keras.optimizers import Adadelta\n",
        "import plotly.graph_objs as go\n",
        "from matplotlib.pyplot import cm\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "import keras\n",
        "import h5py\n",
        "## input layer\n",
        "input_layer = Input((210, 22, 3))\n",
        "\n",
        "## convolutional layers\n",
        "conv_layer1 = Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu')(input_layer)\n",
        "conv_layer2 = Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu')(conv_layer1)\n",
        " \n",
        "## add max pooling to obtain the most imformatic features\n",
        "pooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer2)\n",
        "\n",
        "conv_layer3 = Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu')(pooling_layer1)\n",
        "conv_layer4 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(conv_layer3)\n",
        "pooling_layer2 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer4)\n",
        "\n",
        "## perform batch normalization on the convolution outputs before feeding it to MLP architecture\n",
        "pooling_layer2 = BatchNormalization()(pooling_layer2)\n",
        "flatten_layer = Flatten()(pooling_layer2)\n",
        "\n",
        "## create an MLP architecture with dense layers : 4096 -> 512 -> 10\n",
        "## add dropouts to avoid overfitting / perform regularization\n",
        "dense_layer1 = Dense(units=2048, activation='relu')(flatten_layer)\n",
        "dense_layer1 = Dropout(0.4)(dense_layer1)\n",
        "dense_layer2 = Dense(units=512, activation='relu')(dense_layer1)\n",
        "dense_layer2 = Dropout(0.4)(dense_layer2)\n",
        "output_layer = Dense(units=10, activation='softmax')(dense_layer2)\n",
        "\n",
        "## define the model with input layer and output layer\n",
        "model = Model(inputs=input_layer, outputs=output_layer)"
      ],
      "metadata": {
        "id": "6oPvYTvmDVN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv3D, MaxPool3D, Flatten, Dense\n",
        "from keras.layers import Dropout, Input, BatchNormalization\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from plotly.offline import iplot, init_notebook_mode\n",
        "from keras.losses import categorical_crossentropy\n",
        "#from keras.optimizers import Adadelta\n",
        "import plotly.graph_objs as go\n",
        "from matplotlib.pyplot import cm\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "import keras\n",
        "import h5py\n",
        "## input layer\n",
        "input_layer = Input((100, 66, 1))\n",
        "\n",
        "## convolutional layers\n",
        "conv_layer1 = Conv2D(filters=8, kernel_size=(3, 3), activation='relu')(input_layer)\n",
        "output_layer = Dense(units=10, activation='softmax')(conv_layer1)\n",
        "\n",
        "\n",
        "## define the model with input layer and output layer\n",
        "model = Model(inputs=input_layer, outputs=output_layer)"
      ],
      "metadata": {
        "id": "SBozQWKyp_JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=categorical_crossentropy, metrics=['acc'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "Fx9dTSO3qkRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load datas\n",
        "\n",
        "dataset = load_data('dhg_data.pckl')\n"
      ],
      "metadata": {
        "id": "0gLqYBBE08ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Splitting the dataset for training and testing.\n",
        "def is_test(x, _):\n",
        "    return x % 4 == 0\n",
        "\n",
        "\n",
        "def is_train(x, y):\n",
        "    return not is_test(x, y)\n",
        "\n",
        "\n",
        "recover = lambda x, y: y\n",
        "\n",
        "# Split the dataset for training.\n",
        "test_dataset = dataset.enumerate() \\\n",
        "    .filter(is_test) \\\n",
        "    .map(recover)\n",
        "\n",
        "# Split the dataset for testing/validation.\n",
        "train_dataset = dataset.enumerate() \\\n",
        "    .filter(is_train) \\\n",
        "    .map(recover)"
      ],
      "metadata": {
        "id": "VaJNmfUFz1U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(x_train)\n",
        "X_train = x_train.reshape(323)\n"
      ],
      "metadata": {
        "id": "mqIKOCNX2wZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(y_train)"
      ],
      "metadata": {
        "id": "dr7jGA2D210K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.reshape((100, 66))"
      ],
      "metadata": {
        "id": "bxoEl3Er3bDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=x_train, y=y_train, batch_size=10, epochs=5, validation_split=0.2)\n",
        "#model.fit(x=x_train, y=y_train, batch_size=128, epochs=50)"
      ],
      "metadata": {
        "id": "QiBUzVzn7hp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ceobYPOXmx4e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}