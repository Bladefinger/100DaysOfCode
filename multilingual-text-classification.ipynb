{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/QKMGFCHZH7Y\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","metadata":{}},{"cell_type":"code","source":"!pip install sentence-transformers\n!pip install protobuf\n!pip install deep_translator\n!pip install pandas","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-05T14:53:13.030655Z","iopub.execute_input":"2022-08-05T14:53:13.031379Z","iopub.status.idle":"2022-08-05T14:54:03.180142Z","shell.execute_reply.started":"2022-08-05T14:53:13.031202Z","shell.execute_reply":"2022-08-05T14:54:03.179155Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[K     |████████████████████████████████| 85 kB 339 kB/s eta 0:00:01\n\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n\u001b[K     |████████████████████████████████| 4.7 MB 583 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.55.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.8.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.19.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.24.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.5.4)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.95)\nCollecting huggingface-hub>=0.4.0\n  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n\u001b[K     |████████████████████████████████| 101 kB 3.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.4.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.0.12)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.3.0)\nCollecting packaging>=20.9\n  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n\u001b[K     |████████████████████████████████| 40 kB 2.8 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (2.4.7)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->sentence-transformers) (0.18.2)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->sentence-transformers) (0.6)\nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n\u001b[K     |████████████████████████████████| 6.6 MB 48.4 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2020.11.13)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.4.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.15.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.2)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2020.12.5)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (2.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (1.0.0)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (7.2.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125918 sha256=c0b1e96fdc9be9636c1aa497677e7908b05e11846023ce6e709ccfd96058392e\n  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\nSuccessfully built sentence-transformers\nInstalling collected packages: packaging, tokenizers, huggingface-hub, transformers, sentence-transformers\n  Attempting uninstall: packaging\n    Found existing installation: packaging 20.8\n    Uninstalling packaging-20.8:\n      Successfully uninstalled packaging-20.8\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.9.4\n    Uninstalling tokenizers-0.9.4:\n      Successfully uninstalled tokenizers-0.9.4\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.2.2\n    Uninstalling transformers-4.2.2:\n      Successfully uninstalled transformers-4.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab-git 0.11.0 requires nbdime<2.0.0,>=1.1.0, but you have nbdime 2.1.0 which is incompatible.\nbokeh 2.2.3 requires tornado>=5.1, but you have tornado 5.0.2 which is incompatible.\nallennlp 2.0.1 requires transformers<4.3,>=4.1, but you have transformers 4.21.1 which is incompatible.\u001b[0m\nSuccessfully installed huggingface-hub-0.8.1 packaging-21.3 sentence-transformers-2.2.2 tokenizers-0.12.1 transformers-4.21.1\n\u001b[33mWARNING: You are using pip version 21.0.1; however, version 22.2.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (3.14.0)\nRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.7/site-packages (from protobuf) (1.15.0)\n\u001b[33mWARNING: You are using pip version 21.0.1; however, version 22.2.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nCollecting deep_translator\n  Downloading deep_translator-1.8.3-py3-none-any.whl (29 kB)\nRequirement already satisfied: requests<3.0.0,>=2.23.0 in /opt/conda/lib/python3.7/site-packages (from deep_translator) (2.25.1)\nCollecting beautifulsoup4<5.0.0,>=4.9.1\n  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n\u001b[K     |████████████████████████████████| 128 kB 430 kB/s eta 0:00:01\n\u001b[?25hCollecting soupsieve>1.2\n  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2020.12.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (1.26.2)\nInstalling collected packages: soupsieve, beautifulsoup4, deep-translator\nSuccessfully installed beautifulsoup4-4.11.1 deep-translator-1.8.3 soupsieve-2.3.2.post1\n\u001b[33mWARNING: You are using pip version 21.0.1; however, version 22.2.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.2.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2020.5)\nRequirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.19.5)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n\u001b[33mWARNING: You are using pip version 21.0.1; however, version 22.2.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\n\nimport pandas\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom torch import nn, optim, tensor\nfrom torch.nn import functional as F","metadata":{"execution":{"iopub.status.busy":"2022-08-05T14:54:03.183210Z","iopub.execute_input":"2022-08-05T14:54:03.183692Z","iopub.status.idle":"2022-08-05T14:54:06.630662Z","shell.execute_reply.started":"2022-08-05T14:54:03.183643Z","shell.execute_reply":"2022-08-05T14:54:06.629815Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, embedding_dim, num_labels, dropout):\n        super(Classifier, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.num_labels = num_labels\n        self.dropout = dropout\n\n        self.dp = nn.Dropout(self.dropout)\n        self.ff = nn.Linear(self.embedding_dim, self.num_labels)\n\n    def forward(self, input_embeddings):\n        tensor = self.dp(input_embeddings)\n        tensor = self.ff(tensor)\n        return tensor, F.softmax(tensor, dim=-1)\n\n\nclass Batcher(object):\n    def __init__(self, data_x, data_y, batch_size):\n        self.data_x = data_x\n        self.data_y = data_y\n        self.batch_size = batch_size\n        self.n_samples = data_x.shape[0]\n        self.indices = torch.randperm(self.n_samples)\n        self.ptr = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.ptr > self.n_samples:\n            self.ptr = 0\n            self.indices = torch.randperm(self.n_samples)\n            raise StopIteration\n        else:\n            batch_indices = self.indices[self.ptr:self.ptr+self.batch_size]\n            self.ptr += self.batch_size\n            return self.data_x[batch_indices], self.data_y[batch_indices]\n\n\ndata_df = pandas.read_csv(\"../input/multilingual-spam-data/data-en-hi-de-fr.csv\")\n     \ndata_df.dropna(inplace=True)\ndata_df.drop_duplicates(inplace=True)\ndata_df.rename(columns={\n    \"Category\": \"labels\",\n    \"Message\": \"text\"\n}, inplace=True)\n\nle = LabelEncoder()\nle.fit(data_df.labels)\ndata_df[\"labels\"] = le.transform(data_df.labels)\n\ntrain_x, test_x, train_y, test_y = \\\n    train_test_split(data_df.text, data_df.labels, stratify=data_df.labels, test_size=0.15,\n                     random_state=123)\n\ntrain_x_de, test_x_de, train_y_de, test_y_de = \\\n    train_test_split(data_df.text_hi, data_df.labels, stratify=data_df.labels, test_size=0.15,\n                     random_state=123)\n\nsentences = train_x.tolist()\ntest_sentences = test_x_de.tolist()\n\nlabels = torch.tensor(train_y.tolist())\ntest_labels = torch.tensor(test_y_de.tolist())\n\n# encoder = SentenceTransformer('distilbert-base-nli-mean-tokens')\nencoder = SentenceTransformer('quora-distilbert-multilingual')\nprint('Encoding segments...')\nstart = time.time()\nembedding = encoder.encode(sentences, convert_to_tensor=True)\ntest_sentences_embedding = encoder.encode(test_sentences, convert_to_tensor=True)\nprint(f\"Encoding completed in {time.time() - start} seconds.\")\n\ntrain_batcher = Batcher(embedding, labels, batch_size=16)\n\nnum_samples, embeddings_dim = embedding.size()\nn_labels = labels.unique().shape[0]\n\nclassifier = Classifier(embeddings_dim, n_labels, dropout=0.01)\n\noptimizer = optim.Adam(classifier.parameters())\nloss_fn = nn.CrossEntropyLoss()\n\nfor e in range(10):\n    total_loss = 0\n    for batch in train_batcher:\n        x, y = batch\n        optimizer.zero_grad()\n        model_output, prob = classifier(x)\n        loss = loss_fn(model_output, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f'epoch:{e}, total_loss:{total_loss}')\n\nwith torch.no_grad():\n    model_output, prob = classifier(test_sentences_embedding)\n    predictions = torch.argmax(prob, dim=-1)\n    results = classification_report(predictions, test_labels)\n    print(results)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-05T14:54:55.508337Z","iopub.execute_input":"2022-08-05T14:54:55.508733Z","iopub.status.idle":"2022-08-05T14:58:50.798757Z","shell.execute_reply.started":"2022-08-05T14:54:55.508699Z","shell.execute_reply":"2022-08-05T14:58:50.797087Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/345 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2436216ad5f480488eadda2abedb5bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a7d6bae92f84c13b63339126339d006"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/3.73k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a7a04cadf3e49d6a580b08a91425252"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/572 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50f0cea67c7f45d99bd17c7649115c15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de147d8204a44b0cba9628fa9b00c2c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/539M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5458fc41ea544a68b149c0af3e8ed3d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d5d25aa6a1d468a8e93b73978511d02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdba13571b104c5597027c1562bdf561"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4a8725757b44784a99d776a3ad244b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/447 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8135e402ecbc4e148dad82eaecc281bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"098adef3e71643b78bb7829a365b754b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61c30181383646c7b225fb89ae79e154"}},"metadata":{}},{"name":"stdout","text":"Encoding segments...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49e86b5efd5b4bdab0e75a1dae7b8942"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"347b4eefab2e4a31b67f085fbb9b5c40"}},"metadata":{}},{"name":"stdout","text":"Encoding completed in 161.8462839126587 seconds.\nepoch:0, total_loss:62.56313539668918\nepoch:1, total_loss:34.271434688940644\nepoch:2, total_loss:27.416143734008074\nepoch:3, total_loss:23.724894346669316\nepoch:4, total_loss:20.943754298612475\nepoch:5, total_loss:19.61637761676684\nepoch:6, total_loss:17.498989212792367\nepoch:7, total_loss:16.614907732000574\nepoch:8, total_loss:15.870797763112932\nepoch:9, total_loss:14.891708600101992\n              precision    recall  f1-score   support\n\n           0       1.00      0.93      0.96       727\n           1       0.46      0.94      0.62        47\n\n    accuracy                           0.93       774\n   macro avg       0.73      0.93      0.79       774\nweighted avg       0.96      0.93      0.94       774\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}